"""
QIIME 2 Assistant - Interactive AI for QIIME 2 analysis questions.
Powered by Ollama (local LLM) with QIIME 2 manual as knowledge base.
"""

import os
import glob
import json
import requests
import streamlit as st


DOCS_DIR = os.path.join(os.path.dirname(__file__), "..", "qiime2-manual", "docs")
OLLAMA_URL = "http://localhost:11434"

CUSTOM_CSS = """
<style>
    /* ğŸ± Hide Streamlit default chrome */
    #MainMenu, footer, header {visibility: hidden;}

    /* ğŸ± Tighten top padding */
    .block-container {padding-top: 2rem;}

    /* ğŸ± Welcome card styling */
    .welcome {
        text-align: center;
        padding: 2rem 0 1rem;
        color: #555;
    }
    .welcome h2 {
        font-size: 1.4rem;
        font-weight: 600;
        color: #333;
        margin-bottom: 0.3rem;
    }
    .welcome p {
        font-size: 0.95rem;
        color: #888;
    }

    /* ğŸ± Example chips grid */
    .example-grid {
        display: flex;
        flex-wrap: wrap;
        gap: 0.5rem;
        justify-content: center;
        padding: 0.5rem 0 2rem;
    }

    /* ğŸ± Sidebar minimal */
    section[data-testid="stSidebar"] {
        width: 260px !important;
    }
    section[data-testid="stSidebar"] .stSelectbox label {
        font-size: 0.85rem;
    }
</style>
"""

EXAMPLES = [
    "DADA2ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ±ºã‚æ–¹ã¯ï¼Ÿ",
    "ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ·±åº¦ã¯ã©ã†æ±ºã‚ã‚‹ï¼Ÿ",
    "å†…éƒ¨æ¨™æº–ï¼ˆISï¼‰ã®é™¤å»æ‰‹é †ã‚’æ•™ãˆã¦",
    "Rã§phyloseqã‚’ä½¿ã†æ–¹æ³•ã¯ï¼Ÿ",
    "PERMANOVAã®å®Ÿè¡Œæ–¹æ³•ã¯ï¼Ÿ",
    "åˆ†é¡å™¨ã®ä½œã‚Šæ–¹ã‚’æ•™ãˆã¦",
]

SYSTEM_PROMPT = """\
ã‚ãªãŸã¯QIIME 2ã«ã‚ˆã‚‹16S rRNAã‚¢ãƒ³ãƒ—ãƒªã‚³ãƒ³è§£æã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

## å›ç­”ã®ãƒ«ãƒ¼ãƒ«
- ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æƒ…å ±ã‚’å„ªå…ˆã—ã¦å›ç­”ã™ã‚‹
- ã‚³ãƒãƒ³ãƒ‰ã‚’ç¤ºã™å ´åˆã¯ã€ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«è¨˜è¼‰ã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰ã‚’ãã®ã¾ã¾å¼•ç”¨ã™ã‚‹
- ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«è¨˜è¼‰ãŒãªã„å†…å®¹ã«ã¤ã„ã¦ã¯ã€ãã®æ—¨ã‚’ä¼ãˆãŸä¸Šã§ä¸€èˆ¬çš„ãªçŸ¥è­˜ã§è£œè¶³ã™ã‚‹
- æ—¥æœ¬èªã§å›ç­”ã™ã‚‹
- ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ã£ã¦ã‚³ãƒãƒ³ãƒ‰ã‚’è¦‹ã‚„ã™ãè¡¨ç¤ºã™ã‚‹
- Ré–¢é€£ã®è³ªå•ã«ã‚‚å¯¾å¿œã™ã‚‹ï¼ˆãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«R/phyloseqã®ç« ãŒã‚ã‚‹ï¼‰

## ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…å®¹
{knowledge_base}
"""


@st.cache_data
def load_knowledge_base():
    """Load all markdown docs from qiime2-manual/docs/ as knowledge base."""
    docs = []
    md_files = sorted(glob.glob(os.path.join(DOCS_DIR, "*.md")))
    for filepath in md_files:
        filename = os.path.basename(filepath)
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        docs.append(f"--- {filename} ---\n{content}")
    return "\n\n".join(docs)


def get_available_models():
    """Fetch list of models from Ollama."""
    try:
        resp = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        resp.raise_for_status()
        return [m["name"] for m in resp.json().get("models", [])]
    except requests.RequestException:
        return []


def chat_stream(model, messages, knowledge_base):
    """Stream chat response from Ollama."""
    system = SYSTEM_PROMPT.format(knowledge_base=knowledge_base)
    payload = {
        "model": model,
        "messages": [{"role": "system", "content": system}] + messages,
        "stream": True,
    }
    with requests.post(
        f"{OLLAMA_URL}/api/chat", json=payload, stream=True, timeout=300
    ) as resp:
        resp.raise_for_status()
        for line in resp.iter_lines():
            if line:
                chunk = json.loads(line)
                token = chunk.get("message", {}).get("content", "")
                if token:
                    yield token


def main():
    st.set_page_config(
        page_title="QIIME 2 Assistant",
        page_icon="ğŸ§¬",
        layout="centered",
    )
    st.markdown(CUSTOM_CSS, unsafe_allow_html=True)

    # ğŸ± Minimal sidebar
    with st.sidebar:
        st.markdown("#### Settings")
        models = get_available_models()
        if not models:
            st.error("Ollama ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
            st.caption("`ollama serve` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            st.stop()

        default_idx = 0
        for preferred in ["qwen2.5:7b", "qwen2.5-coder:7b"]:
            if preferred in models:
                default_idx = models.index(preferred)
                break

        model = st.selectbox("Model", models, index=default_idx, label_visibility="collapsed")

        if st.button("Clear chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

    # ğŸ± Load knowledge base
    knowledge_base = load_knowledge_base()

    # ğŸ± Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ğŸ± Welcome screen (shown when no messages)
    if not st.session_state.messages:
        st.markdown(
            '<div class="welcome">'
            "<h2>QIIME 2 Assistant</h2>"
            "<p>ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«åŸºã¥ã„ã¦QIIME 2ã®ç–‘å•ã«å›ç­”ã—ã¾ã™</p>"
            "</div>",
            unsafe_allow_html=True,
        )
        cols = st.columns(2)
        for i, ex in enumerate(EXAMPLES):
            with cols[i % 2]:
                if st.button(f"  {ex}", key=f"ex_{i}", use_container_width=True):
                    st.session_state["prefill"] = ex
                    st.rerun()

    # ğŸ± Display chat history
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ğŸ± Handle prefilled question
    prefill = st.session_state.pop("prefill", None)
    prompt = st.chat_input("è³ªå•ã‚’å…¥åŠ›...")
    if prefill:
        prompt = prefill

    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            api_messages = [
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ]
            response = st.write_stream(
                chat_stream(model, api_messages, knowledge_base)
            )

        st.session_state.messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    main()
