"""
QIIME 2 Assistant - Interactive AI for QIIME 2 analysis questions.
Powered by Ollama (local LLM) with QIIME 2 manual as knowledge base.
"""

import os
import glob
import json
import requests
import streamlit as st


DOCS_DIR = os.path.join(os.path.dirname(__file__), "..", "qiime2-manual", "docs")
OLLAMA_URL = "http://localhost:11434"


@st.cache_data
def load_knowledge_base():
    """Load all markdown docs from qiime2-manual/docs/ as knowledge base."""
    docs = []
    md_files = sorted(glob.glob(os.path.join(DOCS_DIR, "*.md")))
    for filepath in md_files:
        filename = os.path.basename(filepath)
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        docs.append(f"--- {filename} ---\n{content}")
    return "\n\n".join(docs)


SYSTEM_PROMPT = """\
ã‚ãªãŸã¯QIIME 2ã«ã‚ˆã‚‹16S rRNAã‚¢ãƒ³ãƒ—ãƒªã‚³ãƒ³è§£æã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

## å›ç­”ã®ãƒ«ãƒ¼ãƒ«
- ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æƒ…å ±ã‚’å„ªå…ˆã—ã¦å›ç­”ã™ã‚‹
- ã‚³ãƒãƒ³ãƒ‰ã‚’ç¤ºã™å ´åˆã¯ã€ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«è¨˜è¼‰ã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰ã‚’ãã®ã¾ã¾å¼•ç”¨ã™ã‚‹
- ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«è¨˜è¼‰ãŒãªã„å†…å®¹ã«ã¤ã„ã¦ã¯ã€ãã®æ—¨ã‚’ä¼ãˆãŸä¸Šã§ä¸€èˆ¬çš„ãªçŸ¥è­˜ã§è£œè¶³ã™ã‚‹
- æ—¥æœ¬èªã§å›ç­”ã™ã‚‹
- ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ã£ã¦ã‚³ãƒãƒ³ãƒ‰ã‚’è¦‹ã‚„ã™ãè¡¨ç¤ºã™ã‚‹
- Ré–¢é€£ã®è³ªå•ã«ã‚‚å¯¾å¿œã™ã‚‹ï¼ˆãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«R/phyloseqã®ç« ãŒã‚ã‚‹ï¼‰

## ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…å®¹
{knowledge_base}
"""


def get_available_models():
    """Fetch list of models from Ollama."""
    try:
        resp = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        resp.raise_for_status()
        return [m["name"] for m in resp.json().get("models", [])]
    except requests.RequestException:
        return []


def chat_stream(model, messages, knowledge_base):
    """Stream chat response from Ollama."""
    system = SYSTEM_PROMPT.format(knowledge_base=knowledge_base)
    payload = {
        "model": model,
        "messages": [{"role": "system", "content": system}] + messages,
        "stream": True,
    }
    with requests.post(
        f"{OLLAMA_URL}/api/chat", json=payload, stream=True, timeout=300
    ) as resp:
        resp.raise_for_status()
        for line in resp.iter_lines():
            if line:
                chunk = json.loads(line)
                token = chunk.get("message", {}).get("content", "")
                if token:
                    yield token


def main():
    st.set_page_config(
        page_title="QIIME 2 Assistant",
        page_icon="ğŸ§¬",
        layout="wide",
    )

    st.title("ğŸ§¬ QIIME 2 Assistant")
    st.caption("QIIME 2 ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«åŸºã¥ãå¯¾è©±å¼AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ« LLMï¼‰")

    # ğŸ± Sidebar for model selection
    with st.sidebar:
        st.header("è¨­å®š")
        models = get_available_models()
        if not models:
            st.error("Ollama ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ã€‚`ollama serve` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # ğŸ± Prefer qwen2.5:7b > qwen2.5-coder:7b > first available
        default_idx = 0
        for preferred in ["qwen2.5:7b", "qwen2.5-coder:7b"]:
            if preferred in models:
                default_idx = models.index(preferred)
                break

        model = st.selectbox("ãƒ¢ãƒ‡ãƒ«", models, index=default_idx)
        st.caption(f"Ollama ({OLLAMA_URL})")

        if st.button("ä¼šè©±ã‚’ãƒªã‚»ãƒƒãƒˆ"):
            st.session_state.messages = []
            st.rerun()

        st.divider()
        st.markdown("### è³ªå•ã®ä¾‹")
        examples = [
            "DADA2ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ±ºã‚æ–¹ã¯ï¼Ÿ",
            "ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ·±åº¦ã¯ã©ã†æ±ºã‚ã‚‹ï¼Ÿ",
            "å†…éƒ¨æ¨™æº–ï¼ˆISï¼‰ã®é™¤å»æ‰‹é †ã‚’æ•™ãˆã¦",
            "Rã§phyloseqã‚’ä½¿ã†æ–¹æ³•ã¯ï¼Ÿ",
            "PERMANOVAã®å®Ÿè¡Œæ–¹æ³•ã¯ï¼Ÿ",
            "åˆ†é¡å™¨ã®ä½œã‚Šæ–¹ã‚’æ•™ãˆã¦",
        ]
        for ex in examples:
            if st.button(ex, use_container_width=True):
                st.session_state["prefill"] = ex

    # ğŸ± Load knowledge base
    knowledge_base = load_knowledge_base()

    # ğŸ± Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ğŸ± Display chat history
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ğŸ± Handle prefilled question from sidebar
    prefill = st.session_state.pop("prefill", None)
    prompt = st.chat_input("QIIME 2ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„...")
    if prefill:
        prompt = prefill

    if prompt:
        # ğŸ± Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ğŸ± Generate response with streaming
        with st.chat_message("assistant"):
            api_messages = [
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ]
            response = st.write_stream(
                chat_stream(model, api_messages, knowledge_base)
            )

        st.session_state.messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    main()
